{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import theano.tensor as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(token):\n",
    "    # Helper function to mask out non-tokens\n",
    "    if (not token.is_ascii\n",
    "            or token.is_stop\n",
    "            or token.like_num\n",
    "            or token.pos_ in ['X', 'SYM']):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def tokenize(document):\n",
    "    # Tokenize by lemmatizing\n",
    "    doc = nlp(document)\n",
    "    return [token.lemma_ for token in doc if mask(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tagger, parser and named-entity recognition\n",
    "nlp = spacy.load('en', disable=['tagger', 'parser', 'ner'])\n",
    "\n",
    "# Read data\n",
    "DATA_FILE = 'NeutralPolitics.csv'\n",
    "data = pd.read_csv(DATA_FILE).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     what points to no collusion which of these cl...\n",
       "1    the unhrc replaced the un commission on human ...\n",
       "2    when they replaced the terrible commission on ...\n",
       "3    the flores decision flores v reno and subseque...\n",
       "4    the protestors in question are being led bycom...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize data using tf-idfs\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode',\n",
    "                             tokenizer=tokenize,\n",
    "                             max_df=0.90,\n",
    "                             min_df=0.001,\n",
    "                             norm='l2')\n",
    "\n",
    "tfidf = vectorizer.fit_transform(data)\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF (Non-Negative Matrix Factorization)\n",
    "\n",
    "Some people in the field of collaborative filtering refer to this method as SVD, despite it having very little to do with the [SVD from linear algebra](https://en.wikipedia.org/wiki/Singular-value_decomposition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorize with NMF.\n",
    "nmf = NMF(n_components=20,\n",
    "          random_state=1618,\n",
    "          alpha=0.2)  # L2 regularization\n",
    "\n",
    "W = nmf.fit_transform(tfidf)\n",
    "H = nmf.components_\n",
    "err = nmf.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print clusters and exemplars.\n",
    "for topic_idx, [scores, topic] in enumerate(zip(np.transpose(W), H)):\n",
    "    print('Cluster #{}:'.format(topic_idx))\n",
    "    print('Cluster importance: {}'.format(\n",
    "        float((np.argmax(W, axis=1) == topic_idx).sum()) / W.shape[0]))\n",
    "\n",
    "    for token, importance in zip(\n",
    "            [feature_names[i] for i in np.argsort(topic)[:-10 - 1:-1]],\n",
    "            np.sort(topic)[:-15 - 1:-1]):\n",
    "        print('{}: {:2f}'.format(token, importance))\n",
    "\n",
    "    print('')\n",
    "\n",
    "    for exemplar_idx in np.argsort(scores)[-5:]:\n",
    "        print(exemplar_idx)\n",
    "        print(data[exemplar_idx])\n",
    "        print('')\n",
    "\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMF (Probabilistic Matrix Factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_std(x, axis=None):\n",
    "    \"\"\" Standard deviation of a scipy.sparse matrix, via [E(X^2) - E(X)^2]^(1/2) \"\"\"\n",
    "    return np.sqrt(np.mean(x.power(2), axis=axis) - np.square(np.mean(x, axis=axis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns, entries = scipy.sparse.find(tfidf)\n",
    "\n",
    "n, m = tfidf.shape\n",
    "dim = 20\n",
    "\n",
    "sigma = entries.std()\n",
    "sigma_u = sparse_std(tfidf, axis=1).mean()\n",
    "sigma_v = sparse_std(tfidf, axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Naive implementation, will not work.\n",
    "with pm.Model() as pmf:\n",
    "    U = pm.Normal('U', mu=0, sd=sigma_u, shape=[n, dim])\n",
    "    V = pm.Normal('V', mu=0, sd=sigma_v, shape=[m, dim])\n",
    "    R = pm.Normal('R', mu=tt.dot(U, V.T), sd=sigma, shape=[n, m], observed=tfidf)\n",
    "\n",
    "    map_estimate = pm.find_MAP()\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This doesn't seem to work either? U and V turn out completely 0.\n",
    "# MAP is an unreliable point...\n",
    "with pm.Model() as pmf:\n",
    "    U = pm.Normal('U', mu=0, sd=sigma_u, shape=[n, dim])\n",
    "    V = pm.Normal('V', mu=0, sd=sigma_v, shape=[m, dim])\n",
    "    R_nonzero = pm.Normal('R_nonzero',\n",
    "                          mu=tt.sum(np.multiply(U[rows, :], V[columns, :]), axis=1),\n",
    "                          sd=sigma,\n",
    "                          observed=entries)\n",
    "    \n",
    "    map_estimate = pm.find_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sampling is takes a prohibitively long time...\n",
    "with pmf:\n",
    "    trace = pm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Probabilistic Matrix Factorization (BPMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = tfidf.shape\n",
    "D = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as bpmf:\n",
    "    beta_0 = 2\n",
    "    mu_0 = np.zeros(shape=D)\n",
    "    nu_0 = D\n",
    "    W_0 = np.identity(D)\n",
    "    \n",
    "    # Instead of Wishart priors, we use LKJ priors on the correlations, as\n",
    "    # that is more numerically stable: https://docs.pymc.io/notebooks/LKJ.html\n",
    "    L_U = pm.LKJCholeskyCov('L_U',\n",
    "                            n=D,\n",
    "                            eta=D,\n",
    "                            sd_dist=pm.HalfNormal.dist(1))\n",
    "    lambda_U_chol = pm.expand_packed_triangular(D, L_U)\n",
    "\n",
    "    L_V = pm.LKJCholeskyCov('L_V',\n",
    "                            n=D,\n",
    "                            eta=D,\n",
    "                            sd_dist=pm.HalfNormal.dist(1))    \n",
    "    lambda_V_chol = pm.expand_packed_triangular(D, L_V)\n",
    "\n",
    "    mu_U = pm.MvNormal('mu_U',\n",
    "                       mu=mu_0,\n",
    "                       chol=np.sqrt(beta_0)*lambda_U_chol,\n",
    "                       shape=[D,])\n",
    "    mu_V = pm.MvNormal('mu_V',\n",
    "                       mu=mu_0,\n",
    "                       chol=np.sqrt(beta_0)*lambda_V_chol,\n",
    "                       shape=[D,])\n",
    "    \n",
    "    #U = pm.Normal('U', mu=mu_U, sd=lambda_U, shape=[n, dim])\n",
    "    #V = pm.Normal('V', mu=mu_V, sd=lambda_V, shape=[m, dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] https://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf\n",
    "\n",
    "[2] https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf\n",
    "\n",
    "[3] http://www.cs.toronto.edu/~rsalakhu/BPMF.html (code for [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "SQL query for data:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "  -- In order: lowercase, strip URLs, HTML entities, and punctuation, and replace whitespace with single spaces\n",
    "  REGEXP_REPLACE(\n",
    "    REGEXP_REPLACE(\n",
    "      REGEXP_REPLACE(\n",
    "        REGEXP_REPLACE(\n",
    "          LOWER(body),\n",
    "          -- URL regular expression modified from https://daringfireball.net/2010/07/improved_regex_for_matching_urls\n",
    "          r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s[:punct:]]))\",\n",
    "          \"\"),\n",
    "        \"&([a-z]|#)[a-z]*;\",\n",
    "        \"\"),\n",
    "      r\"[[:punct:]]\",\n",
    "      \"\"),\n",
    "    r\"[[:space:]]+\",\n",
    "    \" \") AS text\n",
    "FROM\n",
    "  `fh-bigquery.reddit_comments.2018_06`\n",
    "WHERE\n",
    "  subreddit = 'NeutralPolitics'\n",
    "ORDER BY\n",
    "  LENGTH(text) DESC\n",
    "LIMIT\n",
    "  10000\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
